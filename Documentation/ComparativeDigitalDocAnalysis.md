Comparative Digital Document Analysis
-------------------

A:Craig Duncan

D:12 September 2024

# Licence

See [Licence](LICENCE.md)

# An introduction to Comparative Digital Document Analysis

Comparative Digital Document Analysis is based, in part, on an analytical framework for digital documents that gives priority to the function of digital information (whether it be data, encoding, files etc) that remains within the computing system, in the context of general computing.  It focusses on the process of encoding information in digital form right up to the point it is converted to analogue signals in a standard output device, like a monitor or speaker.  Regardless of the digital form of information, the connectivity with the computer ends when the information leaves the monitor in analogue form so that it can be perceived by humans.  The layperson may be unaware of this lack of continuity, and will *recognise* or project these on-screen representations of documents as if the analogue form has all the qualities of the digital form that produced it.  However, due to the different nature of the digital machine, and human sensory interpretation, what is produced on an output screen always hides digital information of some kind.  Every program that displays output has already incorporated a decision about what level of abstraction is important to the human operator of the machine, because it is a discretion, or choice.

Progressive layers of abstraction make it harder to see this distinction between text that is abstracted at one level slightly below analogue forms of simply decorated text, but HTML is a good example of one.   By giving a person an analogue representation where the tags are not yet interpreted into a cleaner analague form, we can see the deliberate placing of partitions and markers.  These are part of a convention for an input-output (computing) model for how to convert that stream of digital information with deliberate partitions and markers into a much simpler visual output (decorated text) for analogue reading.  

The simplest forms of analogue representations of digital documents are merely convenience, and they come at the cost of not being able to peer too far inside the computer.  If we want to do any kind of conscious computing that integrates with the human mind and perception system, as well as allow us to use text, we have to break the illusion of seamless analogue continuity and enter into some kind of regular encoding of information that will be interpreted by the computer.  Every human tool for working with a computer, in a way that allows us to modify the computer's memory in a way that suits us, seems to involve the need to prepare an analogue representation which peers into a lower layer of data format.   When we want to code on a computer, for example, we are forced to prepare an analogue representation of the programming text, which will involve the use of a programming language that can be used to interfact with the computer.  

Every analogue representation of something like digital text is mediated by a program that decides if it is merely going to display code to a human being, or act on it as an instruction to the computer.  This is always needed to bridge the gap between the mechanical, analogue human, and the digital processor.  A human may be satisfied with being able to read and recognise some analogue information produced by an e-reader software, but the digital representation of that document within the computing system is something else.  E-readers generally disable the ability to peer beneath the computing system that offers analogue representations, because for economic products, there is a choice between what to offer and what to leave out.  Computability is not considered so beneficial when the product being offered is meant to be nothing more than a text read by a passive consumer.

For the layperson, the main distinction, or framing, that is often associated with computerised documents is between electronic documents and non-electronic documents.  There is also framing of preparing digital media as an analogue output for *consumption*, prepared by someone other than the reader.  The framing is strongly influenced by economic models and distinctions: the roles of users/readers are descriptions that often assume that the software producer or manufacturer is the proactive producer of the analogue form of digital media, and has computing enabled, whilst the human readers, whether they be professional information workers or not, are relative passive recipients, with computing disabled, and analogue representations foregrounded.

# Comparative digital analysis is based on a general computing model for information

Comparative digital analysis is not based on an economic model of software production.

My proposal is that we shift our focus, and define digital documents within a computing system by how they fit within a more general model of computing, and include all aspects up to the production of analogue output in deciding whether to say they are primarily focussed on analogue output, or retaining computation functions, or a mixture of both. 

This distinction is important, because it moves away from looking only at digital document functions that involve outputs that are familiar as traditional media.  Rather, I take it as a given that some outputs will look familiar as traditional media, but then ask: despite this, what should we retain or leave out of the digital format that can be interpreted in that way?  We should engage in a broad analysis of the relationships between form and function, structure and purpose.

My aim is to be able to to analyse and define digital documents that provide computable data, data that is used for analogue representation on screen, or perhaps both.  

This includes understanding whether a format like HTML is information-rich or noisy from the perspective of the writer's original semantic data and literary goals.  If information other than the writer's own semantic content is incorporated into the document merely through the publication process, then the end result is a noisy document and inconvenient structures for re-acquiring the information down stream.  However, informed data standards for HTML preparation may make that task easier.  For example, implementing MessengerHTML should make it easier to scrape data from the HTML with something like Beautiful Soup, than HTML prepared in ignorance of downstream uses.  With Messenger HTML, if you turn off CSS you will still have something that contains the same data in its HTML, and is probably very clean and simple in your browser as well.  Your code should, in its HTML form, be clean and readable without too many nested tags or distractions.

# Writing syntaxes designed for analogue-focussed digital output

Even though writing in a digital computer may seem like 'digital' input, if the main focus of the software output is analogue presentation (to a human reader), and it lacks attention to data, then it is likely that it remains analogue-focussed digital software from input to output.  This analytical description helps us become more aware of the fact that the computable functions in software that interprets or transforms using this as the principal syntax will tend not to offer the writer any intermediate computing functions that do not relate to analogue output.

For example, Markdown syntax (John Gruber) is capable of being analysed as a shorthand version of HTML, in that its primary focus is to help the writer provide instructions for analogue layout, but disguising that function by making the text look like it is a set of detective's notes written in an old mechanical typewriter.  This has little interest in making it easier to record the text for data transmission purposes.  However, it has created or perpetuated the culture of focussing on analogue representations as if they defined what a digital document is.  This now extends to analogue-representation converters like pandoc.  The reason that YAML formats are supplementary data sections within Markdown and pandoc Markdown is largely because the analogue-focussed digital format is supplemented with data only where it is deemed necessary, but not because data is a first-level concern of the computing process.

With GUI formats (WYSIWYG), the insulating effect of analogue representations is even greater.  Instead of there being a linear input to output process focussed on analogue representation, the input is also made to look like the output.  The difference between input and output shrinks, so that it all looks like analogue representation.  The writer is, despite the buttons and mechanisms of the digital software, really being asked to pretend they are still using a digital typewriter without data functions. 

# Informed upstream processing

By adopting a functional definition for digital documents that includes some use of writer's data, we can move toward more deliberate processes that aim to achieve retention of semantic information.  This can help with a new kind of definition for what we mean by digital media and a semantic web.

We have a choice, for example, between digital texts that merely use text decorations for standard output, like Markdown, or ones that build-in the expectation of general functions and data definition blocks within a literary of essay-like text.  This allows the writer to work with a program that does not allow analogue representation to dominate the possibility of data and computable text.  This isn't just a change to how we produce analogue output; it is also a change to the extent to which a writer can use computation and numerical tools, or text processing, within the same software that is allowing them to process text.

I am not merely describing an integrated text editor that has extra features.  My initial project is based on the idea that we can prepare a digital document in any text editor and that what matters is having a program that provides an interpretation of its contents.  To the extent this allows data storage and functions, I call this a programming language, and an interpreter, rather than a 'markup language'.  Of course, it has features in common with both markup languages and programming languages, but these are fundamental to computing: data partitions, grouping of text.  These are true of any data format; what matters is what we enable the computer to do with it.

Another interesting possibility of this analysis is that rule-based processing of documents, as a text stream (rather than using a document object model) can be enabled by allowing the writer to use functions that apply to the balance of the digital document (assuming a top to bottom processing direction).  Any programmatic interpretation of digital text files offers the possibility that line by line marking up can be replaced by an automated pattern-matching and replacement process.  For example, we can deliberate program the text interpreter to act on replacement rules that look for particular text, and if it is found, process it to standard output with a particular category.  This auotomatic semantic labelling is very effective for auto-formatting of plays in literature, for example.

Another observation is that even a basic text editor provides some analogue representation, because this is how a human reads it.  However, in programming or coding, it was always understood that this was merely a means to an end, namely the subsequent processing of the crafted text file as instructions for a computer.  From this we should conclude that any digital file can exist simultaneously as both an analogue representation and one that is capable for computation.  

By taking this observation and thinking about how it might apply to the whole process of computing, from input to output, we can ensure that the notion of analogue representation does not become a limitation on how we interpret and process digital texts for any purpose, not merely for software engineering.  For example, in my project, a text file does not fall neatly into the category of a memo or program, or an essay or program.   It is possible to have functions and data blocks in the same 'file' as a play, as well as having two files where the more functional elements are separated.  The point is, that it is not the fact that we can see the document on the screen that is important: what is important is how much the computer environment is prepared to process the file for different functions, and how much the human writer is prepared to take advantage of that deliberate computing model. 