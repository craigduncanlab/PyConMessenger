<html><head><title>AutoTagScripts</title><meta name="copyright" content="Â© Craig Duncan 2024" /><meta name="author" lang="en" content="" /><meta name="robots" content="Index,Follow" /><meta name="description" content="This contains work created, researched and published by Craig Duncan.  Credits to third party sources appear where appropriate." /><meta name="keyword" content="Digital Humanities, digital thesaurus, interoperability, semantic content, languages, semiotics, linguistics, literate computing, computable text" /><meta name="theme-color" content="darkorange"><meta http-equiv="content-type" content="text/html; charset=utf-16" /></head><body id="Top"><link rel="stylesheet" href="fnlstyle_new.css"><p class="title">AutoTagScripts</p><p class="date">Friday, 02 August 2024</p><p class="author">By Craig Duncan</p><p class="concept">Part of a series of essays on theory and methods for data processing of textual information</p><p class="navlink">Prev:<a href="OtherWebWritingMethods.html" class="intlink">OtherWebWritingMethods</a> Next:<a href="SplittingByDelimiterSets.html" class="intlink">SplittingByDelimiterSets</a></p><p class="related"><a href="#PLN1" class="intro">Intro</a>, <a href="#PLN2" class="intro">ATS (autotag script) goals</a>, <a href="#PLN3" class="intro">Consequences for how we view computing and digital writing</a>, <a href="#PLN4" class="intro">Categorical delimiters</a>, <a href="#PLN5" class="intro">Consequential benefits</a>, <a href="#PLN6" class="intro">How it works</a>, <a href="#PLN7" class="intro">Additional commands for preprocessing HTML</a>, <a href="#PLN8" class="intro">Generalisation</a>, <a href="#PLN9" class="intro">Simplification</a>, <a href="#PLN10" class="intro">ATS vs javascript</a>, <a href="#PLN11" class="intro">Semantic text and style processing</a>, <a href="#PLN12" class="intro">Other implementations</a>, <a href="SiteMap.html" class="intro">SiteMap</a></p>
<p id="PLN1" class="intro">Intro</p><p id="P1" class="np">The goal of autotagging is to enable classification and splitting (by delimiters) that reduces the amount of explicit markup is needed.</p><p id="P2" class="np">The means of doing this is by pattern-matching involve units of information at the word level rather than just letters or characters.  Unlike regular expressions that involve character patterns (or tend to), ordinary writing and formatting tends to use words or phrases, combined with punctuation, as categorical content and delimiters.  </p><p id="P3" class="np">The limited use of this insight probably occurs because our analogue processing of information, and what we might describe as 'recognition', does not tend to be linked to a specific academic discipline or grammar.  It is not a large area of recognition within computer science:</p><p class="list">existing regular expression pattern-matching programs like grep were focussed on any kind of regular expression, not that which was of concern to interpreters of literature; and</p><p class="list">search engines and boolean searchers are often not considered identical to word-level pattern matching in documents.</p><p id="P4" class="np">Within the context of visual scanning of documents, our ability to recognise the small sets of variable words that occur within a format or template often goes without attention.  However, if we scale some of the ideas of regular expression pattern mataching to the word level, like 'match on any of these <b>words</b>, instead of characters, we can achieve a useful function for classification.  </p><p id="P5" class="np">The other useful requirement is a simple function-based system which allows writers to incorporate references to lists of word variables and link them to a classification label without the need for complex low-level programming.  This can be achieved by high-level functions that allow the basic arguments needed to match a pattern-matching list to the category to be attributed to them.</p><p id="P6" class="np">The realisation of these ideas takes the form of autotag scripts and data-formatting protocols that allow simple markup to help define the sets of variables to be used for pattern-matching.</p><p id="P7" class="np">For pre-processing benefits of AutoTagScripts see <a href="SplittingByDelimiterSets.html" class="intlink">SplittingByDelimiterSets</a>.</p><p id="PLN2" class="intro">ATS (autotag script) goals</p><p id="P8" class="np">ATS is designed to make it easier to work with data, to manipulate and preprocess data of any kind that can be put into text.  </p><p id="P9" class="np">The advantage of using ATS is that it will replace many processes that would ordinarily require laborious text tagging using markup.  It will also do things that markup cannot - like reference sets of data to determine when markup will be applied.  It is capable of using semantic information to assist with tagging (i.e. it aims to leverage what an author knows about their own document to automate tagging and classification, rather than ignore that information).</p><p id="P10" class="np">It can change the way you work by allowing you to type in things according to more natural language formatting conventions e.g. just type a play up as usual.  Instead of trying to fit the writing to many different and new formatting or tagging requirements, the idea is that you specify some of the implicit semantic assumptions of those formatting conventions, and can do so knowing that there is a system that will recognise them and do the formatting for you (as well as retaining the kind of semantic categories that you want to use).</p><p id="PLN3" class="intro">Consequences for how we view computing and digital writing</p><p id="P11" class="np">If semantic information can be easily detected and tagged, it opens up ways of allowing computable queries of many ordinary digital text documents without sophisticated techniques: it can leverage a small but focussed set of semantic information for great advantage.  It uses what humans already know rather than time and energy expensive machine learning (which is a bulk attempt at problem solving, rather than a nuanced, insightful one).</p><p id="P12" class="np">This also opens up ways of framing our perspective of what digital documents are, outside of the 'document object model' that has been used for so much of web and word processing analysis to date.</p><p id="PLN4" class="intro">Categorical delimiters</p><p id="P13" class="np">I have written about <a href="CategoricalDelimiters.html" class="intlink">CategoricalDelimiters</a> elsewhere but here is a summary:</p><p class="list">Still a productive area of research for me.</p><p class="list">Builds on earlier thinking on the nature of documents and how we delimit data units in things like mathematics and computer science</p><p class="list">Delimiting and classification in the context of long-form textual information is slightly different than quantitative units or short categorical data.</p><p class="list">The main difference is that we must scale-up previous theory (regex etc) to fit with ordinary language level words and labelling</p><p id="P14" class="np">The basic theme or thread running through all of this is that humans (unlike most computer science conventions) use words as both delimiters and classifiers when designing templates and formats for literature and professional documents.  This can be found in things like:</p><p class="list">Names that are re-used to introduce new dialogue in a play or legal transcript</p><p class="list">Agreed spellings and salutations for things that form a pattern of use</p><p class="list">Terms of art in different disciplines being used as a prefix</p><p class="list">Small sets of abbreviations for things like journals, books that indicate a reference</p><p class="list">The use of a small set of stage directions or verbs for describing entrance/exit of actors in a play</p><p class="list">Page numbers at the bottom of a page can also be interpreted as both a delimiter and semantic information (place in the sequence)</p><p id="P15" class="np">In addition, the units of information are often found in lines or paragraphs of text which often have some structure, but often determined by contextual use of grammar, punctation and words.  Rather than trying to solve this as a general predictive problem, it is much easier to focus on what scripts and semantic data will provide a solution in each case, or that provides a semi-general solution where the same kind of format is used all the time.</p><p id="P16" class="np">Once semantic data and function solutions to automatic tagging are found, they can be recorded and stored in autotag scripts.  </p><p id="P17" class="np">There is a benefit in reducing the process of classifying ordinary text document information to pseudo code or a simple programming language.  By providing a high level abstraction of the relation between semantic data and data processing functions, it is easier to understand what is being done (and which was always explicit to literature publishers, even if only implicit to computer scientists).  The autotag scripts (even as pseudocode) provide a descriptive resource that provides an insight into how semantics and structure are related in a particular context.  These scripts can be used for education, and in a practical sense, can be used multiple times for the benefit of everyone.</p><p id="PLN5" class="intro">Consequential benefits</p><p id="P18" class="np">Once auto-tagging becomes relatively sophisticated, then other options for output (publishing) can be more easily introduced e.g. if notes can be easily tagged using a custom semantic delimiter, then it is possible to introduce an option to hide/unhide those categories for the purposes of output.  Even simple binary filtering, using this function, can make it easier to deliver clean or annotated documents in different settings.</p><p id="P19" class="np">Some other examples of applications for binary filtering include: providing questions only, or questions and answers; providing one language only, or a two-language (translated) version; providing factual material only, or facts and commentary.</p><p id="PLN6" class="intro">How it works</p><p id="P20" class="np">ATS works in conjunction with functions that use that semantic data to perform data-cleaning operations (from a computer science perspective, implicit structure is made explicit: to everyone else it is translation of what they already knew)</p><p id="P21" class="np">Once this classification is completed, it then tags it in an XML-type format.  This is then used to produce HTML.</p><p id="P22" class="np">THe ATS parser carries out line-based pattern matching in order to split, merge and classify ordinary text.   It does this using a small set of markup prefixes and tags, but it also uses any user-provided semantic information as delimiters and classifiers, found either in the main text document or in a separate ATS file.  </p><p id="P23" class="np">This ATS is the major difference between forcing people to choose only those delimiters that are familiar to computer scientists (like commas, tabs etc).  The basis of ATS is that  <a href="CategoricalDelimiters.html" class="intlink">CategoricalDelimiters</a> are now an additional piece of document data that is very useful to automating the tagging process.   </p><p id="PLN7" class="intro">Additional commands for preprocessing HTML</p><p id="P24" class="np">The basic pre-processing idea, for plain text files, now includes separate commands (as part of a simple script language) for these things:</p><p class="list">page meta data (author, date, title, concept) : in form of prefixes</p><p class="list">data importing and manipulation</p><p class="list">creation of images and tables from data</p><p class="list">page link creation (HTML production)</p><p class="list">auto-tagging of semantic content (ATS)</p><p id="P25" class="np">Independently of these, there are:</p><p class="list">data blocks (c,e,d,a,l and quote delimiters) which can be used to quickly format text in different ways; and</p><p class="list">a config file which can specify general choices in relation to numbering etc.</p><p id="P26" class="np">It is my intention to turn some of the config options into ATS commands so that they can be altered for individual documents, contexts etc (like the line numbering, spacing, default names for things).</p><p id="P27" class="np">Some of the pre-ATS tags that I developed, like table() and image() were designed to turn content (which overlaps with semantic data) into tables, lists or captioned images (for HTML purposes).  However, they were mainly data-parsing functions, rather than classification functions, in that they read the data into memory <em>before</em> any HTML was produced.  In this way, users or authors could produce HTML without having to use layout-based markdown or HTML, and could re-use data read in during this process in different ways.  This separates concerns: user data is not tied to the document object model that governs javascript and HTML, and so frees users from having to think about the semantics of HTML, and concentrate only on their own subject semantics.</p><p id="PLN8" class="intro">Generalisation</p><p id="P28" class="np">By allowing for an ATS file to be written separately, a writer does not have to include the general auto-tagging functions with their writing if they do not want to.</p><p id="P29" class="np">An ATS file can be setup with more than required for an individual file (e.g. semantic lists could include the complete set of tags that will be needed).  If the list entries happen to be case sensitive (e.g. capitalised), chances of false hits in other files would be lower.</p><p id="P30" class="np">ATS files might be created so that there is one general file e.g. import(generalPlay), and then a smaller set of commands or data entries for specific documents.</p><p id="P31" class="np">ATS is based on the idea that semantic data is the missing link between analogue documents and explicitly structure documents that meet more general computer science ideas (notation, languages) about how to partition and encapsulate data.  We need to capture semantic data that is relevant to particular forms of literature or professional writing, in a form that computer scientists can more readily use in a pre-processing step to automatically determine what needs to be tagged, and how it is to be tagged.</p><p id="P32" class="np">ATS allows semantic information to be contextualised (the scripts accompany the author's text) and then used to prepare implicitly structured information (from a computer science perspective) into a more explicitly structured form (based on computer science assumptions that tagging rather than the use of ordinary words as both delimiters and classifiers is the 'norm').</p><p id="P33" class="np">Through this mechanism, domain-specific and discipline-specific tagging can be done without the burden of having to do XML tagging.</p><p id="PLN9" class="intro">Simplification</p><p id="P34" class="np">Autotag script removes the need for complex regular expression statements by using the fact that many ordinary documents in literature, or law use ordinary words as delimiters and classifiers, and that they are often used in a way which lends itself to a split or merge operation.  By abstracting some of these implicit ways of organising information in pre-digital literature, we can make them more explicit in a computing context, particularly for pre-processing what computer scientists would call unstructured data, and which everyone else knows is structured data.</p><p id="PLN10" class="intro">ATS vs javascript</p><p id="P35" class="np">Unlike javascript which deals with the 'document object model' of an HTML page (and therefore uses and references HTML tags as if they were the primary data types, or objects), autotag is based on processing and classifying an author's text without any preconceived notions of what HTML is.  It is effectively working out how to create an XML file first, which is then converted to HTML, using the semantic tags as the classes for the HTML.</p><p id="PLN11" class="intro">Semantic text and style processing</p><p id="P36" class="np">i.e. <em>CSS sheet relationships and custom classification.</em></p><p id="P37" class="np">We can make the semantic pre-processing of text independent of style names (but not in any way making HTML or CSS any more complicated), by introducing methods of linking styles to a standard list of CSS styles, with the least inconvenience to a writer.  My initial implementation allowed someone to 'block' out a list of items (which was a time-saving compared to markdown, which required each line to be formatted), but it didn't change the name of the formatting in CSS from 'list'.  However, if the user can specify what semantic name they want for a list, and still link it's style to list, there is not substantive difference in what is produced.  </p><p id="P38" class="np">For example, a 'list' can be a type of data (a p tag class) that is used as the base CSS format for several semantic styles.  Instead of the p class name just being 'list', in the CSS file we replace p.list with whatever semantic category we want to duplicate the 'look' of list.  In the CSS file that would be "p.list, p.particulars12" etc.  </p><p id="P39" class="np">Semantic information requires us to specify names for things in our source data.  In our semantic data, we can define a list using say an 'l' block and include an optional(?) variable name with that list (instead of markup stopping and starting with just the format of that list, we allow the user to specify the semantic name they want to give it too).</p><p id="P40" class="np">Perhaps I should use an i+ block for image definitions, which then allow image data to be read in and given a name that is different from source if required.</p><p id="P41" class="np">TO DO:The difference between an 'l+' tag and a 'd+' tag may be that a list can be automatically processed into paragraphs without the need for a separate 'list()' command?  If a variable name is present than there should be a modification of the CSS sheet to add any additional 'l' variables to that formatting list.  Then in the HTML generation, the 'class' will not be 'list', but whatever the user has specified.</p><p id="PLN12" class="intro">Other implementations</p><p id="P42" class="np">I suppose it would be possible to define the data in an ATS file (with 'd+' tags etc) in a JSON file (or similar), that defines the variable and then the list of items for each variable.</p><p id="P43" class="np">Alternatively, it could be like R markdown where the variable name appears in {} in the first line of a ``` block?</p><p id="P44" class="np">What if the .ats file was itself a python file?  i.e. by specifying the variables and data for each, this could then be a python module which was opened and used, providing variables when required?</p><p id="P45" class="np">How are lists etc defined in R?</p><p id="P46" class="np">The actual 'commands' part of an ats file could be implemented by a function call.  JSON files don't do this, unless the commands became a data section (e.g. JSON entry that it is called 'commands')</p><p id="P47" class="np">I want this to be relatively easy for people to use, and not have to learn JSON etc.</p><p id="P48" class="np">Finally, the command could invoke some function call in a language other than python?  This would require porting to javascript or ruby or something else?  </p><p class="footer"><a href="SiteMap.html" class="footer">Site Map</a> | <a href="#Top" class="footer">#Top</a></p><p class="footer">Created by: Craig Duncan 2023-2024</p><p class="footer">Except where otherwise noted, content on this site is licensed under a Creative Commons Licence: <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"class="footer">CC BY-NC-ND 4.0</a></p></body></html>