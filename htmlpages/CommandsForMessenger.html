<html><head><title>CommandsForMessenger</title><meta name="copyright" content="Â© Craig Duncan 2024" /><meta name="author" lang="en" content="" /><meta name="robots" content="Index,Follow" /><meta name="description" content="This contains work created, researched and published by Craig Duncan.  Credits to third party sources appear where appropriate." /><meta name="keyword" content="Digital Humanities, digital thesaurus, interoperability, semantic content, languages, semiotics, linguistics, literate computing, computable text" /><meta name="theme-color" content="darkorange"><meta http-equiv="content-type" content="text/html; charset=utf-16" /></head><body id="Top"><link rel="stylesheet" href="fnlstyle_new.css"><p class="title">Commands for Messenger HTML</p><p class="date">Wednesday, 24 July 2024</p><p class="author">By Craig Duncan</p><p class="concept">This could be called 'text programming' or 'literature programming'.</p><p class="related"><a href="#PLN1" class="intro">Prologue</a>, <a href="#PLN2" class="intro">Where markup and HTML got it wrong</a>, <a href="#PLN3" class="intro">Rule-based auto-tagging of styles</a>, <a href="#PLN4" class="intro">HTML features that leverage semantic classes</a>, <a href="#PLN5" class="intro">A domain-specific programming language in this context</a>, <a href="#PLN6" class="intro">Files for programming and literature</a>, <a href="SiteMap.html" class="intro">SiteMap</a></p>
<p id="PLN1" class="intro">Prologue</p><p id="P1" class="np">The mission of libraries and archiving foundations like the Utah-based Project Gutenberg is to make literature available in a readable form.  It's not a requirement for this kind of project that literature be made available in a particular format, or even a semantic one.  However, thinking about semantics means that we can also think more explicitly about the relationship between traditional components of literature, human readability and semantic significance. </p><p id="P2" class="np">A collection of Michael Hart's writings is available at the next link.  Michael was the creator of Project Gutenberg, back in the early 1970's.</p><p class="related">ExtLink:<a href="https://hart.pglaf.org" class="extlink">MichaelHartWriting (accessed 25 July 2024)</a></p><p id="P3" class="np">In particular, some of his views on the priority for making literature available in different formats, any readable format, is here:</p><p class="related">ExtLink:<a href="https://hart.pglaf.org/media.07.txt" class="extlink">MichaelHartInternetBooks (accessed 25 July 2024)</a></p><p id="P4" class="np">I don't disagree with those general ideas, but I like to think about how the infrastructure for digital writing was taken for granted, and so, like many things that seem uncontroversial, it can be quietly changed to make it harder to do things that once seemed free, and simple.  </p><p id="P5" class="np">My interest in semantics is mainly about explicit recognition in digiral literature of the same things that were in pre-digital literature, or taken for granted when we read something, and making them a little bit more explicit.  It is not intended to shut down people's ability to be creative about how they do that.  In fact, I want writers and publishers to be able to choose whatever semantic scheme seems right, and not to prevent others from having a different view.</p><p id="P6" class="np">We are now in an age where people are baffled by how to keep modifying HTML to put in more semantic content, and also how to insist on semantic categories in web pages.  These are, in many ways, leading to an authoritative way of encoding for the internet.  In my view, they are too prescriptive.  The kinds of tags for microformats are designed to work with corporate search engines.  They are tools for the system.  On the other hand, there are a lot of crazy entrepreneurs and conspriacy theorists trying to make a buck out of Web3 by putting everything on a blockchain, which is far from the simplest web you can imagine.  </p><p id="P7" class="np">Semantic categories of literature are still relevant to presentation, so an effort to understand document semantics is not wasted when it comes to how we publish in digital formats.  By encoding and retaining semantic information, we ensure readability for both humans and computers.   This is because semantics is often the basis for style, but not described as such.  </p><p id="PLN2" class="intro">Where markup and HTML got it wrong</p><p id="P8" class="np">The assumption of markup is that a human being would hand-craft the individual, line by line styles for subsequent transformation into HTML.  This made people slaves to HTML.  There are some situations where this is useful (where they only want to use a small set of categories), but it mainly caters for people who want to fit within some HTML convention.  The more people want to say what they want, the more HTML has felt the need to expand, by adding more tags.  This is too prescriptive, and it will never be enough.  You can't have one system of classification for the world with stifling freedom of expression and thought.</p><p id="P9" class="np">What we can do is think more deeply about providing computer assistance to both semantic categories and layout.  This requires some data to let the computer know what sorts of things are going to appear in a document, perhaps several times, and will still be the same kind of information.  This then allows the computer to do what it is good at: rule-based saving of effort. In formal areas of literature, law, where there are often word formulas and patterns like "Enter Romeo" or "In these particulars,...", then rule-based semantic classification is even easier.</p><p id="P10" class="np">It is relatively easy to demonstrate that rule-based styling can be in addition to or substitute for markup.  As such, it is relevant to both reading and computation.  </p><p id="P11" class="np">The way that rule-based classification of text can be implemented is an example of constructive multiplication of data.  For example, it can leverage the fact that summary sections, or table of contents are, in fact, the basis for describing the styling of similar information in the remainder of the document (i.e. they are independent data, which is then used to automate styling).  </p><p id="P12" class="np">On the other hand, word processing often implies reductive inference, to create a table of contents from a serious of headings.  Rather than save the writer the effort of classifying and styling the contents, it actually requires the writer to make go to a lot of effort to style each and every paragraph heading (even if this involves selecting from pre-formatted styles), just to get a table of contents.  </p><p id="PLN3" class="intro">Rule-based auto-tagging of styles</p><p id="P13" class="np">We accept for now that there is a structure within publishing formats, that allows for rule-based processing of text.  This allows the introduction of relevant data encoding and functions that can help with automatically tagging (or annotating) literature (like plays) with particular categories of text, and styles.</p><p id="P14" class="np">This is achievable because we can notice patterns and consistency in syntax or line formatting, and use this to create functions that will apply those rules when we want to use them.  These functions can achieve this in many ways, often by detecting the occurrence of a word from a specific set, or a pattern, and also by distinguishing between an entire line that is a match from features within that line, like matched text.</p><p id="P15" class="np">This approach is no different to ordinary programming, but by making a small set of functions it simplifies the kind of 'programming' that an ordinary writer needs to do.</p><p id="P16" class="np">Once sets of functions seem to be working, they might even be able to be converted to generalised functions, or simplified further.</p><p id="PLN4" class="intro">HTML features that leverage semantic classes</p><p id="P17" class="np">Page links can be generated for information that belongs to certain headings (e.g. scenes, acts etc).  These only need to be first classified, by the text/markup parser, then they can be separately the subject of a writer instruction to make page links based on those.  In those ways, even the layout of the web/document can be more easily handled by the writer than using a content management system.</p><p id="P18" class="np">Content management systems make many choices about what types of information, including content, are important to a theme, but disguised as questions of layout.</p><p id="PLN5" class="intro">A domain-specific programming language in this context</p><p id="P19" class="np">Being able to use simple functions to program text and use it as data based on semantic categories (sets, groupings) can assist a writer who wants to publish. We can make implicit structure explicit by using commands that are regular and consistent.  </p><p id="P20" class="np">It also means that texts can be effectively 'programmed' into layout as a secondary stage: we do not have to markup if we have a sensible basis for classifying text, as we do in plays.  </p><p id="P21" class="np">The design of a programming language that helps frame a text in terms of semantic sets (e.g. by reference to sets of data that are of importance to someone in a particular field) is also an aid to communication of what information is being structured, and how.  This will help overcome the impression that literature is somehow not structured.  It is mainly because it requires an additional step to make implicit structure explicit for computation that computer scientists have regarded ordinary writing, even forms of literature, as unstructured.</p><p id="PLN6" class="intro">Files for programming and literature</p><p id="P22" class="np">By allowing text and commands to co-exist in the same file we can achieve data and function separation whilst also recognising that visible text is data, and so we can deal with it as visible data (in a text editor) and then separately confirm it is to be published.  </p><p id="P23" class="np">Our working text does not need to immediately become publishable text.  This is an overlooked constraint of word processing: word processing doesn't allow us to use text files as sandboxes or allow us to play with data that may or may not be intended to be published in that form.  Where the text is partly data (as in tabular data, or lists etc) then we can use it in several ways, including:</p><p class="list">As the basis for a table;</p><p class="list">As a list of data to be turned into another format;</p><p class="list">As data for a function for preprocessing text (e.g. lists of characters, or verbs etc that we want to use to tag lines of text or individual words).</p><p id="P24" class="np">There is a time saving in being able to write rules that will instruct the computer to apply the same rule to marking up a file thousands of lines long.   This is not only more efficient than markup, but it enables the classification of files suitable for XML and overcomes the stated objections to using that format because data 'entry' is too time inefficient.</p><p class="footer"><a href="SiteMap.html" class="footer">Site Map</a> | <a href="#Top" class="footer">#Top</a></p><p class="footer">Created by: Craig Duncan 2023-2024</p><p class="footer">Except where otherwise noted, content on this site is licensed under a Creative Commons Licence: <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"class="footer">CC BY-NC-ND 4.0</a></p></body></html>