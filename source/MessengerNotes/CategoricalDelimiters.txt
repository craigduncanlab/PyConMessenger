T:Categorical Delimiters in Digital Literature
A:Craig Duncan
D:31 July 2024
C:Key concepts for code-enabled digital literature

# History

The focus on making authors of data fit their markup or delimiters to a preconceived standard (e.g. csv, XML, HTML) originated in computer science, where they originally responded to the need to fit data to the machine's need for partitioned data in a Turing style machine.   One of the foundational assumptions was that delimiters and data had to be different (for ease of effecting branched instructions in the machine: i.e. detecting a special character indicated immediately that a data-processing function, like splitting the data etc had to be performed).

This preparedness to draw a distinction between content and delimiters is at the heart of computer science (and has persisted despite the clash between use of a comma in comma separated values and comma in ordinary grammar).  

The introduction of more complex delimiters (like XML tags) was a step toward recognising that the delimiters themselves might also contain more information, but it was yet another artificial format in response to the perceived ways that digital machines needed to process information.  It was treated as the starting format for processing of semantic information, rather than an intermediate point.  This thinking has continued to influence the idea of the semantic web, in that the explicit tagging of information is seen as a necessary pre-requisite to machine processing.  It has continued to influence the idea of 'structured' data being data that is explicitly (often manually) organised with delimiters that stand outside the text.

This narrow view of delimiters and the way in which different types of information are specified has continued to influence how we look at certain types of information as unstructured simply because they use delimiters, or formatting systems, in which the delimiters themselves are words positioned on a page.  The human visual system reads these and interprets them so quickly that they escape attention as a delimiter system that also uses a non-tagged form of semantic encoding.  The other thing that humans use to do this is to appreciate the significance of certain sets of words (i.e draw on categorical knowledge), in order to appreciate the function as both a delimiter and a form of semantic encoding.  Book covers, title pages and many other forms of information are both encoded and structured in this way.  Computer scientists immediately think that some form of additional, explicit tagging or delimiter encoding is needed to structure them.

All sorts of analogue-based structures (including both handwritten mathematical notation as well as literature formats) might have delimiter systems that can be immediately appreciated by a human reading but also have multiple encoding rules operating at the same time.  The focus on fitting data formats to the simplest methods, namely those that involve a single data channel that partitions data (mostly) has discouraged looking at the way in which existing digital literature (based on analogue formats) actually uses delimiters.  Increasing complexity in the computer domain has involved creating hiearchical, explicitly-tagged, nested data structures rather than recognising that in human writing, the delimiters are also data too; thereby using words to fulfil different functions like data partition, data type, and content.  

The often confusing situation we are now in is that there is a lot of software dedicated to hiding the structures that are used to help present information as if it were simply an analogue form (i.e. for reading).  There is no need for those structures to make explicit any of the categories that the authors were using to adopt them in the first place. As a result, there are multiple formats for the web, and for text readers.  The semantic schemes are neither those of the author's choosing, nor are they capable of being universal, in that they were originally designed to use a few common types of graphic elements that would often attract attention, or would signify importance (e.g. size, bold type for 'headings').  The result of this is that structures for encoding publication on the web and other formats are more focussed on what will grab human attention, than on the data, or meaning, of the content.

This focus on attention-grabbing tagging has largely ignored any attempt to record the categorical set for which the attention-grabbing elements were recognised to the author. It is largely assumed that the human readers will work that out for themselves.  Independently of this, the semantic web gurus are still trying to come up with their own, novel scheme of categorising the world, hoping that human writers can somehow fit themselves within it.  The naivety of this scheme is that it is trying to classify information which is already self-evidently meaningful to human readers by a different method.  It is trying to come up with an equally effective but novel scheme that does not recognise how humans have already, rationally, structured their own content.  It kind of resembles an effort by a linguist to develop a mathematical scheme to interpret a language they assume they will never be able to speak or understand.

The way out of this paradoxical dead-end is to refocus on how traditional literature, like plays and transcripts are already structured, and then to meaningfully interpret the data in this way.  We can then answer questions like 'who is speaking' and 'what did they say' with relatively simple code.

# Categorical delimiters - Summary

This is part of a general topic: 'data analysis in the digital humanities'

In traditional computer science, a delimiter is often a single character or token that acts as a data divider (like a comma in comma separated values). In literate texts the concept of a delimiter may expand in at least three ways:
l+
firstly, it may comprise a set of alphabetic characters that perform the same function, rather than just one delimiter; 
secondly, it may even be a set of words that act as delimiters; and 
thirdly, the delimiters may perform both a partitioning role, and a classification role.
l-

In a play, the Character names are a set of delimiters for the data in the document, if you widen your viewpoint so that words can act as both categories and delimiters. 

Traditional computer science aims for a simplicity that is more reductive than the kinds of data formats that analogue literature uses.  Ironically, these formats have been considered unstructured, when in fact they are more deeply structured.  It requires a more explicit awareness of the way in which they work to understand how semantic information is integrated into data partitioning.

Categorical delimiters are sets of words performing the same functions of partitioning data and also characterising it.  That is, a delimiter can be something that is also part of the data.  Categorical delimiters simplify our appreciation of data manipulation in the context of literate digital documents.  They can be indicators of the cateogry to which an entire line/block of text belongs, as well as being a convenient point to split that data into smaller pieces.

To work with categorical delimiters we need an introductory section of the document (or a shared module that can be used for similar documents), in which the sets of delimiters specific to that data can be defined. e.g. character names for that play are defined.  We then have a rule that tests to see if those delimiters/classifiers appear at the start of a block, or in an individual line, and defines what category should apply if they do.  This rule can take the form of a function or programming line that prescribes how that data-classification (auto-tagging) rule will work for the remainder of the document.

It turns out that categorical delimiter sets are very common in digital literature, so much that they can be considered as standard data type for the data analysis of digital literature.  And, in conjunction with this, functions that use the presence of categorical delimiters are a way to enable automatic classification of a unit (line/block) of text.  That is, a function which refers to categorical delimiter sets is an effective way to carry out auto-tagging in a digital document.

Resort to markup schemes (to publish in HTML etc) is based on the idea that documents are unstructured and so require manual crafting of each and every unit (block of text).  This is a premature admission that we cannot compute the structure that is so readily apparent to us in analogue terms.  By failing to understand the inherent encoding in analogue documents, we have two human generations of software that has encouraged markup, either in the sciences in text documents, or in the humanities in Word processors that require manual styling (and yet, which are also determined to hide the semantic nature of that explicit encoding scheme from us).

Auto-tagging (rule-based semantic classification) procedures save the need for manual encoding of literature that involves repetitive patterns (as all 'forms' do).  It reverses the approach of manual word-processing, in that it takes the 'contents' summary and uses it for the repetitive encoding, instead of only doing half the job: word processors provide users with the ability to apply a category, but do not supply a rule for saying when that will occur.  They do not offer the opportunity for rule-based, categorical delimiters and classifiers.

!!In theory, with a sufficiently broad knowledge of documents, auto-tagging could routinely format and semantically classify documents based on (a) hypothesis about what the set of semantic delimiters are (e.g. pick up the character names in a play from the format itself, then as a secondary step, apply a style/semantic category for those elements via auto-tagging).!!

Using expert domain knowledge, the semantic analysis of text could at least proceed in a staged, rational way, in order to extract information.  It relies on recognition of the kinds of information that are present, through the form, and then the ability to retrieve those kinds of information when requested.  All the normal functions expected of structured data (e.g. being able to recall the variables for a specific instance that relate to a generic data type or label) should be able to be performed on digital literature without requiring the writers or readers to undertake deliberate restructuring, provided that the decodign process can take place systematically, in a useful sequence.

The assistance that can be offered in a programming environment is to enable functions that simply and accentuate the relationships between human methods of communication, where significance of position, word, font etc are all present at same time (take chat threads too, where 'reply' has a particular meaning, encoded by 'referring back' to the source, but which is not captured when serial archiving is undertaken).

e.g. The usual significance of information appearing under a heading is to (a) merge it into same type of data and (b) link it to the category of the heading.  Making it easier to specify this 'rule of interpretation' with reference to form requires (or is made easier by) categorical delimiters.  These enable rules to explicitly capture what is implicit in the original human formats.  i.e. semiotics of digital documents (at present time) are used to avoid unnecessary restatement of that significance to the computer (which is what the 'semantic web' project basically requires).

# Program language requirements

Summary:
l+
Ability to define a categorical data set (i.e. user defined variables, category sets)
Definition function to specify the delimiter-classification relationship: a category set for future auto-tagging, and the data class that category delimiters signify.
Parsing function to check for existence of delimiters in a given set, and allocate class to that set (e.g in anticipation of preparing new data format or publication format)
l-

Once these tasks become more explicit, software design can be better structured to take them into account.

A more simple tag classification definition function might simply specify the delimiter/marker as an outer bracket (of specific or general kind) and what class is to be applied.  This then defines how to classify the output once detected.

! The more that auto-tagging is used, the less critical it is to process explicit markup in the initial text file.  Rather, auto-tagging can use rules to create XML directly.!!

# High level observations

If the programming language that can handle layout and auto-tagging is, in effect, describing the semantic scheme that the author used, then the ability to answer questions about the contents is enabled by the same process that guarantees some kind of sensible style-based layout?  i.e. in the process of performing an intelligent description of salient high level information (like the characters in a play), you are also enabling data that would then be immediately available to someone who queried the original source format.  i.e. what some might regard as meta-data, or contextual data, is being specified in a data format.   

"The semantics dictates the styling" means that semantics is already available for queries (interrogation).

This is the incentive to encode relevant semantics, in a relevant way.  It's currently not part of the semantic web model, because that model thinks that it is just providing a schmea for data pages in a model of a document that is based on passive layout features.  The semanitc web idea is currently not based on an operational model of a document (i.e. both data and program combined).

# Query capacity

Can we define semantics sufficiently well to then ask: what are the stage directions in Act 1, scene 2?  Does the structure of a play map sufficiently well to the document format that we are using?  Do we have an equivalent way of answering what does 'in Act 1' mean? i.e. can we select write a function to select text in between two semantic markers ?

What we are really aiming for is a semantic format that can easily encode for this, and then retain the ability to publish in analogue forms as well.  in our digital formats, we should apply the principle of !*digitally-useful, but 'back-compatible' with analogue*! , rather than the goal of !*any-way-you-can make it look analogue is ok.*!

Similarly, if we have formats that are traditionally written for analogue intepretation (like when Courts require transcript to be written in a way they like), can we then add some programming code that effectively defines the manner of interpretation of that document, thereby providing the 'semantic sugar' that will enable the same digital format (even when just appearing as plain and explicit text without any hidden tagging or delimiters) to be used as a data source for computation and, if necessary, to subsequently style it?

# Semantic preprocessing language (SPL)

What I am proposing is that we have an additional semantic programming language that handles both data and functions, and thereby allows us to add or markup the relevant semantic data that is implicit in an analogue text format.  In other words, we have a suitable and thus efficient logic for think about the relationship between semantic categories and traditional data formats.

The addition of relevant semantic information as a data type to digital documents helps us make sense of traditional analogue text formats when found in digital documents.  This digital treatment of analogue formats has too-often been considered unstructured (rather than being, in fact, very intentionally structured for analogue interpretation, which also uses semantically-grouped content for interpreting that structure).

Previously, I had all of the instructions required to specify some semantic data types and mappings inserted in the text file (i.e. where relevant to define information for images or tables as it arose).  There is no reason why this information could not be predefined in a separate text file.   In this way we clearly distinguish between data and layout (something that markup/markdown languages do not).

If we move the data input for image information into a separate file, it means that we need to store image data and an image name for each image in memory, rather than just use a temporary memory-store for a single (latest) image, in a sequence of commands.  

The more we move preprocessing into a separate data-storage process, the more we need to hold variable names for the data.  i.e. when we just process one image at a time, as needed, we have no need to store it more permanently (saving on memory as a whole), but it adds volume to the original data file.  Someone who wants to write quickly and design for layout could simply write 'image (imagename)' as a shorthand for including a preformatted/pre-read image into the relevant output/publication format.  We could even read in, and encode, a batch of images with the same author information, but with different captions.  

By adding an explicit and customised semantic section we enable both data-gathering/input and formatting.  For traditional literature, this makes a writer more aware of how the format uses data in the form of abstract categories (like Characters, Locations), even before modern computing was invented.

Rather than simply being meta-data in a layout format, it is semantic data that !!enables!! HTML (layout) output, whilst also adding explicit details that bridge the gap between human-style analogue interpretation and computable media.  The 'semantic bridge module' or 'semantic definition section' is that section added which contains code and/or data definitions and/or classifier definitions that enable the rest of the document's data format to be interpret easily by computer programs, with no additional changes.

Perhaps, the easiest way to add a semantic module is to import the semantic-language setup and code using 'import X' at the top of an ordinary text document, which then tells the program what semantic interpretation language information can be added.  In some cases, like a play, the play data already overlaps with semantics, so you can put the dramatis personae in the semantic section, and then use a simple 'publishlist(listname) command in the main document to repeat it in the finished HTML.

Semantic modules can be localised or made relatively generic in some cases to be used with several instances.  It might be possible to include them explicitly (just like javascript in HTML) or in a separate linked file.

Here is an example of what I added to a plain-text form of a High Court transcript to enable the semantics to be understood:

d+
(CourtForm)
IN THE HIGH COURT
d-

d+
(Parties)
PLAINTIFF
MINISTER
d-

d+
(Speakers)
HER HONOUR:
MR HILL:
MS COSTELLO:
d-

d+
(Orders)
The 
d-

d+
(Notices)
AT 
TRANSCRIPT
ON
d-

autotagline(Notices,concept)
autotagsplit(Speakers,authorref,np)
autotagline(CourtForm,intro)
autotagline(Parties,intro)
outertag(concept)

Now I could add the 'import()' command to the processing of the text file and have all that included in a separate file.  This would keep the semantic additions separate to the original content.  It might also enable some kind of general file (e.g. with a growing list of speakers or parties) to be used instead of individual files.  Also, the capitalised conventions of the documents can be a useful feature for semantic interpretation.

This process is different from XML and markup, in that it utilises the semantic information to preprocess an existing analogue format into XML and then HTML, in such a way that the semantic categories can be preserved.

It's also possible that an existing document with formatting issues could be corrected, from scratch, with minimal effort, by taking a plain text version and adding appropriate semantic commands.

# File format proposals.  

Maybe .sem can be used to indicate which text files can be 'imported'.  e.g. import(file) would be understood, if no extension, to mean 'file.sem', and this pattern could be used to find relevant files.  If it cannot be found an error can be generated.

The demarcation of an import function might mean two-step processing.  ie. the import file has to be processed first, to update the parser's memory.
