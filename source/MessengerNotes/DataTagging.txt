T:DataTagging
A:Craig Duncan
D:2 August 2024
C:How do the d+, e+ etc blocks work, and why? Are lists any different?
F:ScienceWritingNotation

# Intro

Explicitly tagging sections of text is not often done in markup or markdown (but code/comments sections and quote sections are similar).  When it is done in word processing, it is associated with manual GUI operations like 'select' or 'apply style to selection'.   This kind of language and framing reflects the lack of interest in renaming and taking ownership of the operations as a form of data definition.  Data-typing and labelling is occurring, but with not real desire to help authors benefit from this.

Even in a plain text environment, once we start to insert markers for text block sections, we might initially think we should distinguish types of blocks. e.g. data blocks (with further pre-processing contemplated) are different from lists and quote sections (where the only processing involved is to add {CR} for each line, then tag the whole section in XML ready for some HTML conversion later).  However, this perpetuates the distinction between 'data' text (for computation) and 'layout text' (for publishing, analogue representation).  This binary between 'digital' information and 'analogue' information is false, because all information in a computer is digital.  What matters is what is done with it, and this is not an all or nothing proposition.  Digital information can sometimes be processed to a point of analogue output, but there are many instances where multiple other, or intermediate computations are useful.

For the above reasons, we should consider all these tagged/blocked sections of text as data.  A list is data, even if the author is only explicitly thinking about putting dot points in front of each entry for analogue output purposes.  A list may be a set of points related to a topic or semantic category.  Even examples in a definition term are part of a set of data that should be able to be accessed in a computer search, independently.  If the author has gone to the trouble of listing that information, then the list should be retrievable.  When this list then becomes the assumed knowledge for further expansion or explanation, it might help to refer back to the original information (this is often implicit in analogue representations of texts but there isn't enough awareness of the benefits of making this explicit within a digital language in the same digital system that stores the information).

Treating all blocks of text as line-based data allows authors or ourselves to pause between the 'parsing' of these sections, and the generation of output.  The intermediate step is where logical contemplation, or computation of what is in memory at that point can occur.  This intermediate data recognition or definitoin step is one of the biggest omissions from the word processing software workflow: the software just doesn't anticipate any further utility from marking (or styling) operations, and so it does not expose it to the author as a form of data definition, only to its internal processes aimed at analogue representations.

Exposing the author's own data to them can be achieved through a data definition language associated with the act of writing.  This includes one that can remember data definition operations as well as give the author an opportunity to carry out book-keeping (e.g. naming of variables) that both the act of data definition and helps educate the author about its ongoing significance.    In this way, a writer who unconsciously structures their writing can become more conscious of what they are doing, and thus take greater control of how information is created and re-used.

There are some professions where attention to the definition of data is emphasised more than others.  A computer programmer, for example, is trained to learn a language that will explicitly define data types and elements, on the basis that this has ongoing utility.  The same profession is involved, generally, in preparing software for other disciplines, outside their own.  As a result, software is often written from an outsider's perspective, and this continues to occur despite efforts to bring 'user-centred design' into programming.  

The ongoing but underappreciated problem is that user centred design often involves listening to the functional requirements of users of software, without inviting those other perspectives in as a critical comment on the assumptions and discourse of computer science itself.  When the graphical interface and user experience become the interface language for formulating user specficiations, it continues to encourage a discourse in which the computational elements provide the !*magic methods*! to deliver machines that are always black boxes to the users.

Within the humanities and digital humanities, the position is different.  Within the humanities, structure in writing was often a self-evident process that was integrated with the familiar human analogue reading/cognitive system.  To anyone in the humanities, it never required explanation because this processing was self-evident.  In some professions like law, the interactions with documents in the electronic age were more frequent, and the archaic language was being scrutinised.  Form and function were sometimes scrutinised as forms of 'plain English' or 'clear writing' and so on, but this was treated as an adjunct to the humanities, !!despite!! the use of computers.  It was, inevitably, partly influenced by the tools that were introduced to help people create typed rather than manuscript documents  (markup and word processing software mainly).  However, the technology was largely used to facilitate a certain look on the page, without the benefit of any new language that deconstructed what was going on in terms of sets of data, and the separation of data from layout.  

The integration of digital form with the tools for achieving analogue publications encouraged people to use the tools as if they were type-setting devices, rather than computers.  The adjective 'electronic' still continues to dominate discourse in professions like law, as a binary counter-example to their traditional paper documents.  No one has seriously defined what electronic means here, but it should be uncontroversial to say that it is not really used in the physical sense of an interest in electrons, electrical signals and so on.  It is as basic as saying produced by something that operates on electricity.  In that sense, it is a term used by lawyers as if they were outsiders to the twentieth century, not the twenty-first.

Bearing this in mind, everything that involves software to a profession that regards it as merely 'electronic' is in distinction to some mechanical method.  Electronic mail is in distinction to paper mail.  Electronic typing or type-setting is in distinction to paper-based (mechanical) typing.  Electronic type-setting has, as a result, become the main mental model for the way in which people relate to word processors.  Even the most sophisticated, complex and graphical software is still largely directed to type-setting.  Within a type-setting environment, classification of content does occur, but if any act of labelling, naming or tagging is immediately associated with narrow end goals (analogue publication) and not with any intermediate data processing or logical operations, then this discourages and prevents the user from using the computing device to assist with computation.  No sooner is the data defined than it is appropriated for the analogue end use and further logical contemplation is discouraged and avoided.

The legacy of this simple framing is that tools for writers never fully expose the local concepts by which writing is information or data.   It is only indirectly approached, through means such as offering an analogue simulation of putting text on a page.  The term 'word processing' could just as easily be described as 'electronic word arranging', so superficial is the treatment of the entire workflow from thought to paper. Any underlying digital format is treated as primarily for the administration of the software, not for the benefit of the writer.

The failure to describe software for writers, or the media in which they work as 'computational documents' means that it fails to receive adequate attention.  This even extends to the internet and world wide web protocols, where HTML and other markup languages still fill a space reserved for 'electronic typesetting', and the semantic web consortium continues to miss the point of what tools for writers would require, by insisting on a universal ontological scheme for representing knowledge (a constant goal of the universalising computer science discipline).

The description 'digital documents' is also not that far removed from "electronic" in terms of its formation of a simple binary.  It recognises that a common explanation for what a computer is to an outside is something that works with 1's and 0's.  This is a legacy of the early days of computing, when it was constructed in terms of its electronic design.  However, at the level of abstraction that is important to someone in the humanities, the difference between digital and electronic is largely irrelevant, because it doesn't work with the abstractions of computer science that are independent of the electronic medium, and relate to the abstract elements of computational logic (which are often, but not exclusively, more mathematical in nature).  In fact, the progressive abstraction of the interaction between people and computers, into higher level languages that allow simple, text-based representations of those 'digital' (binary) operations, is a far more useful description of computing to outsiders.  This form of abstract representation of computing, even in 'pseudo-code', is probably more familiar to programmers.   It is not merely about algorithms, a trending term, but about deciding what data is, or can be, and how to classify it.

Within computing, a 'programming language' is a term that still harkens back to when computing was about writing problem-solving code, that enabled some program of work to be undertaken on a computer.  The really useful part of that term is 'language'.   In needing to work with a device that can be framed at multiple levels of generality (from electronics, to binary operations, to coded instructions, to languages, to graphical displays on visual electronic devices and so on), the most important concept for inter-disciplinary interaction is this idea of language.  To programmers, the world is often subjected to kinds of abstraction in which the world is progressively, hierarchically patterned so that it can ultimately be fitted within one of these programming languages.

The design of computing languages can serve the interests of those who work with computers, but it can also serve the interests of those who want to communicate better with those who work on computers (programmers, software developers) as well as those who want to work directly in that language to achieve their own computing goals.  In relation to the humanities, what we see as a trend is that a lot more time has been spent communicating through visual interface specifications than has been spent designing domain specific languages which happen to be related to a large domain (like writing).   Software engineers and computer scientists write general languages by choice, because they want their own experience of programming (in the most general, universal sense) to be easier.  Some choose to write domain specific languages as if they were helping work with software already based on visual interface requirements, or helping to query a database.  

The trend is that few computer scientists write new programming languages or scripts to help other disciplines work with data from their own field, or to make it more explicit how that other field uses data in an analogue world that can be immediately described in an abstract computing language, even if the way in which it is reduced to some publishing output is not yet defined.   There is a much greater focus on writing software tools, or visual interfaces that help to satisfy the work habits of writers (i.e. those that borrow almost exclusively from the analogue, pre-electronic age).

In summary, the bridge between disciplines, in a computing sense, is as wide as the difference between electricity and paper, because we haven't focussed on making computing languages that bridge the gap between data that is actually used in the humanities as distinct from the data that computer scientists want to see.  The latter is called 'structured', the former is deemed only worthy of a 'database'.  By channeling all ideas about data used by non-computer scientists into the frame of a database, the focus is taken away from the development of languages that articulate the data needs of outsiders to the computing profession.  There is less focus on assisting writers in a way they can determine the form in which their data finds expression in a computing medium.  Digital or coding literacy is only expressed in terms of outsiders to computing learning the languages used by computer scientists to service outsiders.  It perpetuates the idea that computing languages are more often generic, so that they can be adapted to any situation.  Knowledge of these languages is largely a badge of honour by those who work hard to acquire knowledge of computing and electronics.  It also expresses ideas about elegance and brevity that appeal to those with interests in mathematical logic.

Ironically, the attraction of LLM-AI has been that computer scientists can not only continue to work with computing languages and servicing outside requirements for visual interfaces, but that they can lay a claim to being superior at dictating that content that lies outside the computing profession as well.  The underlying desire for universal control systems that motivates many computer scientists has suddenly seemed possible at the ordinary social level, which they were often denied access to because of simple binaries between humanities and computing.  They have also taken advantage of the ignorance of outsiders to having the capacity to do more with less, if they simply had their own programming langauges to work with.  As a result, cumbersome, energy-sapping methods of LLM-AI and appropriation of the content of others, are becoming high-demand investments of computer entrepreneurs.  This continues their distinterest in making tools for self-sufficiency of other professions, in relation to customised programming languages that serve their needs directly and appropriately.  

Those within those professions (like law) were naturally dubious of the need to learn coding (in the traditional sense) because they always perceived it as a technical skill far from their own comfort levels, skills and interests in life.  What they were unable to do was explain how they might request new tools that allowed them to be both writers and logical programmers without too much inconvenience.  The efforts at trying to encode semantic information in the humanities has always been presented as a semantic problem (e.g. it's tedious to make XML from UN resolutions, so lawyers would seek answers from programmers in computer science that involved 'natural language processing', or "AI" or some other complex, generic tool that constructed solutions, often painfully, from programming languages at some distant level of abstraction).

Like most forms of interpretation and cross-over between facts and principles, or facts and logic, there is a need for forms of language that approach the same level of generality, or in which a decision is made to fit specific details in one language within the general label or concept in another.   

To overcome the need to be able to easily move the specific content of documents (think of it as factual or detailed information) into more generic semntic categories that are useful, the implicit structuring of such information that occurs in written forms of writing can be moved toward an explicit writing step so long as authors know that there is a predictable pattern in which of their writing is capable of being automatically classified (auto-tagging) and which requires them to undertake some simple manual tagging.   This task is made much more amenable to conceptual-level thinking if we require the abstraction of this discourse to talk about data and data types, or pseudo-code, rather than confine it merely to the form of publication of such data.   It is not about 'applying styles' or 'defining bullet point lists', but about characterising information that is of a certain kind.

Reducing the conceptual distance between data at the level that computer-scientists think and the level at which humanities professions think is made possible by allowing the humanities professionals to take the relatively easy step of encoding those semantic categories that are important to them, whether it be in general, or in relation to particular documents.  By preparing autotag scripts that define, in a custom language, what semantic categories already exist and what to do with them, we help record recipes for processing computable documents that can be re-used.  Some patterns of language may be so common that one script with only minor variations may be suitable for processing large sets of documents. 

This bulk processing of classification of semantic information by simple recipes in a humanities programming language is probably best realised when looking at literary genres and institutionalised writing.  Institutionalised writing illustrates repetitive processes involving authorities like the Courts,  and the United Nations.  

In relation to Courts: When documents are routinely formatted in the same way, cue words are used that are both delimiters of data and categories.  It is not necessary to ask authors to explicitly tag in XML (as the LegalXML project once contemplated) so long as the rules by which encoding can take place are known (already, based on analogue forms), or can be formulated with minimal adjustment to actual forms and protocols that anticipate the use of auto-tagging.  For example, the word formula "THIS MATTER WAS ADJOURNED AT" is both a delimiter and a categorical signifier for a data type, and so no explicit tagging is necessary.

In relation to the UN: lawyers have asked people how they can obtain the semantic information in decisions (what was decided, how to keep track and so on), and if natural language processing can extract this information and so on.  However, the problem solving paradigms that are in use by computer scientists involve conventional logical pathways, like "can we convert this into XML first" (i.e author-explicit tagging), or "can the computer know, from the sentences, what is being said" (natural language processing).  Those two extremes don't offer practical solutions, because the most direct path (let the author tell you) isn't able to be put into a computable form.  

And instead of enabling the first of these solutions, to enable the author (in a realised sense) to encode semantics (by writing tools and conventions and programming languages) that are more author-friendly, it is simply aborted (see Legal XML for its complexity etc).  The key here is 'realised'.  The problem is not that the authors haven't already put semantic content into their documents; it is the fact that this is not recognised.  It is not recognised because there is a prevailing paradigm in computing that traditional literature cannot be 'read' or interpreted easily to bring it within data concepts, or programming language concepts, that computer scientists are familiar with.   This mental block fails to recognise what is already there: it influences both the education and literacy of non-computer scientists with working in a digital medium (and missed opportunities).  It influences the inability of computer scientists to work on the right problem and change the environment in which people are working.  

If we want to use documents to acquire public interest data (like aspects of Court decisions, UN resolutions), then if the author or some other subject expert can write in a way which presents that information to the general public in a computable document, then it is better than having to do random searches, or data entry by non-experts who have to read and interpret, or do something time-consuming.  It is not merely that these are menial tasks, but that it is the waste of human lives to duplicate effort when the digital mediums are supposed to allow communication (and this should include the communication of insights, or the filtering of such information).

see /UNDocuments/

Any one that goes to the trouble of describing Legal XML should also be enabling an author (or some generic script file) to specify what are the triggers for making XML automatically, and freeing the writer from the irrelevant demands of computer scientists for a format that happens to make it easier to read in semantics (one which, to some degree, will ignore the author's own semantic structuring for an analogue format).  It's the blindness to the inherent analogue structure inside a conventional document that makes computer scientists choose a solution that disregards the information already there, as a result of the initial efforts of the writer.   

No one likes to do extra work solely so a computer can process something: so we need to meet somehwere in the middle.  A small change of habits for a large gain, that is what is the goal here.

The utility of a language like autotag scripts is that there is an agreed language by which semantic categories are mapped to the classification process, and it can be separated from the main text: thereby allowing prediction of what aspects are automated, and giving writers an insight into what additional tagging they might want to do for their own purposes (or because it is easier).

# The role of the author in computable documents

If writers are treated as intelligent programmers of a certain type, i.e. authors of not only content, but its semantic categorisation, then the tools developed for them will be focussed on more intelligent work methods.  The tools will include programminng languages, because writers naturally work with words, and enabling them to dictate the words that relate to the computing of their documents changes the existing paradigm.  The existing paradigm is that a 'word processor' is a software machine that enables processing of the mechanical input of words onto a page, for publication.  It presupposes that the writer of the words is not interested, in any way, in their words as data, nor does it assume that a writer can be empowered in a way that avoids being dependent on some separate database or computer infrastructure designed without concern for how they use information.

(This needs some reframing of the user/author as someone who is not merely wanting a graphical simulator of an analogue device, but who is other than a computer scientists, web developer or database administrator).

# Computable documents

The very concept of a 'computable document' is something that is neither a paper document nor merely an electronic document (fit for analogue use, however complex the underlying software), but a third kind: one that is able to expose the data in the document to both the author and the computer, and allows the author to take some role in deciding what is semantically important, and how that can be both captured and used.  A computable document can save the writer some effort, by allowing the writer to describe patterns in the format o ftheir writing, as well as describing data.  Even when compared to markup (HTML included), the extent to which simple definition of data is better than merely type-setting codes is larger than tends to be known, because it enables even things like tables and lists in HTML pages to be constructed using computer programs and not by the writer.  It enables the writer to declare their intentions regarding publication having already specificed what is semantically important.  We should not be led to think that HTML is a 'semantic' language in a sense that matters to a writer, because that adjective is often used by computer scientists with reference to the end goal of publication and layout, not data definitions.

A focus on computable documents is one which offers any author an opportunity to represent their writing as data, or compute with it as an alternative to simply rendering it on page or screen in an analogue fashion.  It allows them to use some kind of programming language or grammar to work with their ideas.  The complexities of making graphical user interfaces, and the gravitational pull of the analogue counterparts to visualised writing, means that it is probably more productive, conceptually, to allow writers to be able to describe their intentions in regard to their data in programming terms first.   

All of this is to say that 'computable documents' are documents that could easily be in a form that already exist (paper or electronic), but which are currently prevented from being realised by a lack of suitable software tools that use or create the relevant kinds of abstraction.  What remains is to build software and other tools with an awareness of the role of the author, and a motivation to allow the writer a role in semantic classification.  This should not be confused with the semantic web or other descriptions of semantic that computer scientists have invented.  What we are concerned with are custom editing environments and programming/scripting languages for the humanities, that focus on semantic classification.  This will help overcome any of the obstacles that computer scientists, on their own, have failed to solve, or have tried to solve using methods that involve unintelligent processing and which avoid the role of the author.

If the last few decades have taught us anything about discourses across disciplines, it is that framing requests in terms of features for graphical software can be easily met, but it does not guarantee that the disciplines of software and any other field understand each other better.  It has not encouraged the humanities to move toward a better understanding of how they might design their own programming languages to express what they are doing when they are laying out a play.  In some ways it is too obvious to them to bother, but if you ask whether they can turn that knowledge into something that is capable of describing such a situation to a computer programmer, in order to make a language that avoids them using word processors and markup to work in a manual, laborious way, the true difficulties start to emerge.

# Reframing for language design

One way to begin to bridge this language design is to start to think of forms and templates as the distillation of data types within the humanities, that occurred even before there was computing.  Rather than simply call this type-setting, it needs to be recognised as a form of data delimiting and categorisation.  It is part of the language of what matters within a particular field.  In the context of theatre, stage directions, character lists, locations and so on are important concepts for the discipline, and form a semantic framework within which discourse takes place.  Those in the humanities require simple software tools with which they can define these elements so that when they write a computable document, 

It is always possible to redesign software to interact with texts differently, and anticipate a writer who wants to do this.  If we allow an author to ascribe a variable name to the list when it is blocked in the original file, it enables re-use of that semantic data, which is thereby captured and not lost.  The simple step of including some semantic data with lists, even the simplest lists typed into a text editor, means that we can start to work up some general scripts to do more intelligent things with different kinds of information.

The more mathemtical computer scientist, even those that work with language, concentrate on 'processing' some form of meaning with the language, or designing 'data formats' because these things always reduce language to its most basic, mathematical form.  They are also overly-biased toward trying to have representations of truth or knowledge held within a computer in a way which is universal and objective.  This need permeates projects like the the semantic web and, more recently, the appropriation of individuality to some general word-machine like what is called LLM-AI (where the A should be 'asinine', derived from an 'ass').  

Those who work with traditional analogue texts in the humanities have not been recognised as worthy peers in the area of computing, because there are fewer computer scientists who are prepared to be literary data experts.  The writers are framed as doing 'desktop publishing' or similar.  The people that have not been adequately serviced are those that are careful and intelligent writers, or lawyers, or darists or bloggers, all those that write plays, or poetry or legal submissions.  They have been given tools described as word processors, and few have challenged or unpacked the denigrating assumptions within that term. 

The inability to accord importance to frameworks of knowledge which use knowledge subjectively, and locally, and diversely, creates an environment in which software never allows authors to define their own semantic frameworks, hold them in memory, and utilise them for both authoring and computation.  This level of real intelligence has always been dismissed, and is one reason why computer scientists use terms like 'artificial' intelligence, because they want to universalise the notion of thought.  The most recent attempts are attempts to universalise language (originally by taking vast sets of training data; most likely this will narrow).  Every effort, no matter how localised, is an attempt to infer certain patterns based on historical information.  It is merely another case of trying to fit a hypothesis to data, and then to use the hypothesis as if it were proven based on the first patterns detected.

In the past, permitting a computer to take instructions from a person about what to do with the data was considered programming, and programming was often considered the sole concern of people that were not interacting with analogue documents and templates.  Unless something was immediately, obviously and explicitly defined in a mathematical programming language, it was not considered !!structured!!.   The same narrow focus has led to the idea that when people are concerned with directing the end-look of a publication using a computer, they are just performing some 'markup', and this symoblic tagging is done with a 'markup language'.  

What intelligent authors are doing is not merely writing, or writing in an unstructured way.  Very often the language they use is domain-biased, or politically strategic.  It may involve a carefully prepared, hierarchical sequence of ideas.  Intelligent writers will always perform a lot of data definition, because analogue documents by intelligent writers always involved this struturing of thought and ideas, recorded concretely on the page in a form that represented it in an analogue form, suitable for seeing or interpreting through sound.  These analogue documents are not unstructured documents in a general sense: they are unstructured only relative to what computer scientists would prefer, if they want to avoid the messiness (or the layered logic) of those who work in the humanities.  

By at least treating certain blocks/lists of information as data first, we can also write code that can work with these abstractions even in the context of auto-tagging.