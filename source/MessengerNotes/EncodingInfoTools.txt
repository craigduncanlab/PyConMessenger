T:Encoding information
D:4 August 2024
A:Craig Duncan
C:Part of a series of essays on theory and methods for data processing of textual information
B:SplittingByDelimiterSets
F:DataBlockConventions
# Intro

Things to write about:
l+
structure of encoding writing
difference between writing to publish (purely layout markup) and meta-data
meta-programming (i.e. including semantic information for the benefit of both computable documents and layout)

the way in which "AI" has been conceived (what 'means' to that end?  generational attempts)
lack of attention to providing a collateral 'semantic' programming environment that doesn't aim to struture texts irreversibly but offers a conventional interpretation (e.g. analogue conventions about structure, which are themselves cultural indications of a type of literature, language, law etc can be encoded within computers in ways that are both a separation of concerns and a functional addition to the digital tools with which we process, query and distribute information)
l-

Central theme of research topics: !!overlap of written language, type-writing,literature, linguists in the idea of 'encoding' information.!!  Deconstructing what Whittaker did with her poetry (see /TranscriptEncodings/), how this fits with ideas about cultural, semantic norms, and how we might represent the source information in different ways.

Need to think about literature as 'text', an embodiment of coded communications which then adds many unconscious layers of structure within visual systems, and so there is a need to unpack both the visual encoding and the cultural encoding.  Digital systems take analogue systems and add another level of software encoding, which further determines an explicit type of encoding hidden from the analogue reader/viewer, but which may continue to influence the relation between person and software.  i.e. the freedom of authors to deal with (i.e. manipulate, making meaning with, reason with) semantics is diminished in a digital medium that, even though complex, is fundamentally organising information for the end goal of pulication rather than analysis.

Although markdown looks superficially like an aid to the author, it too embodies the layout semantics of HTML, which are themselves part of an ontology of what makes a 'document' rather than one purpose is being served by documents in a particular context.

# The expert insider

The more we pay attention to logic, grammars about the semantic content of documents and the more more we try and develop a programming language suited to computing and the humanities, the more we will understand that nature of that information from an author's perspective, rather than from a software developer who is merely trying to aid an author in manually crafting a digital document to look like an analogue document.  

i.e. software models have been that these are tools for overcoming obstacles to realisation of the same output as analogue media (a weird, backward looking proposition).  i.e. software developers looking at building software as a service to others (the economic, transactional, 'othering' of digital media/tools production.

Within the economic, service/product framing, the making of digital tools does not need to analyse computers as themselves appropriated by the humanities.  Rather the computers and their software were being given to the humanities as if the humanities never had a claim to them.  (user-centred design is still outsider-based design)  This is because outsiders treated this as a demand on their skills in making simulators or tools to reproduce an existing cultural expectation.   

For traditional computer programmers, educated in the sciences, approaching design as if the outsider perspective is natural is much easier, despite the complexity, than tryig to understand what the medium of computers might mean if we focussed on how the humanities might 'compute'.  The answer to that question (cf Rosemary Huisman: Literature, Art & Media) is to seek to understand how much of digital technology could capture conventions to writing, or use them.   

The missing piece of this analysis is the one that asks: how would a person who 'programs in the humanities' do so?  How would we answer this question if they never considered themselves an artist who is an outsider to computing, or a computer programmer who was an outsider to the arts?  We're seeking to understand what the role of expert insider looks like.  The expert insider - how would they view the web, and computing and so on?  This person hasn't been accommodated (and so must basically work out their own tools, to demonstrate what they would like to see). 

The expert insider does not seek to immediately employ the AI tools offered to them by outsiders because such tools lay too much of a claim to understanding the needs, economically.  Simple tools made by insiders may achieve more, because they are suited to purpose, and also have the goal of elegance and directness that more general bulk tools do not.

# Collateral documents - separation of concerns

One way to draw attention to what can be included in our analysis is to study the interaction and relationship between semantic programming/markup/auto-tagging and the kind of expert-level information.  Rather than treat this as 'expert system' (i.e. outside the purvey of computer science), we treat this as insider-information, and we build systems that treat this as natural - we provision for this, and provide programming langauges so that it does not require sophisticated systems buitl by computer scientists as outisders (the AI/Expert paradigm reflects this).

# Recap on categorical delimiters

One of the noticeable consequences of reasoning in a way that tries to normalise the programming role, and to put things back into the 'computer science' box, is that even if humanities' texts are structured within that discipline, there is a powerful urge to recentre or resite the data format as one that requires explicit, non textual delimiters as the data format.   

There's an insecurity about even calling texts that are routine in the humanities 'structured', and the consequence is that they become rewritten with explicit delimiters, and even then categories are yet another 'step' in reasoning.

This blindness to analogue structures that are in the digital realm and still carry significance means that the tools that help reason about humanities structures don't readily exist.  Let's say that structure can be horizontal (data categories across a line,but within a paragraph e.g. PERSON:Speech and dialogue).   The urge of computer scientists is to split this and put it into an array, or some other internalised structure.  yet it is already in a form that has a delimiter (provided you can specify what it is).  It's no different to pairs of data, in a comma separated list, where the first term is categorical and the second is a string of dialogue.   Then there is vertical structure: the ending of single units of data with the LR/CF at the end of a line, or the continuity of data types until a new data type is found.  Computer scientists are looking for brackets, curly brackets or other kinds of grouping conventions to introduce into these texts.

Since the humanities make language part of the categorises and delimiting process, it is natural to recognise and attribute significance to it, so much so that it ought to be the subject of a programming language.  Also, humanities package information for 'books' by pages that are bound, and this is a unit of concrete representations for books, but it doesn't meant that there are not other forms of structure that are independent of the page.  This becomes more important when formats like plain text in Project Gutenberg do away with page numbers in some cases and just reproduce text.  On the other hand, the analysis of structure for texts does not take into account the hierarchical and sequential manner in which author's and publishers introduce structure:
l+
structure that is inherent in the text, even if in plain text; and
structure by splitting into pages and book form.
l-

An analogue publication uses both. A digital representation of an analogue representation implicitly uses both, because human readers of analogue conventions are familiar with the context and so read semantic structure whilst also being aware that pages are regular breaks throughout.  yet the software that deals with these documents doesn't take time to separate these concerns.  The splitting of text into smaller, line-based units occurs, as do page breaks and numbering (used in the analogue world to make references or speak to a location).  The digital book is a little like a sequentially-numbered slide show (the text itself performs as if an image).  It doesn't attach the author's significance to the contents in a way that can be accessed independently of viewing.

Software developers are so busy trying to answer the call to build machines for typesetting analogue documents that they do not know where to start when it comes to building a new model of computation from the ground up for reasoning about the contents.  Programs like Adobe Design are working with the flow of text from resizable boxes, but not the semantic content.  Text is treated as if it were a physical medium, a volumetric medium that ocupies a certain space on the page.

The advantage of a programming language that works with types of information introduced to break up the text for the arts is that it helps navigate the written form of text.  Take an example like plays, which would otherwise be a performance: sound, space, actors, costumes, location etc.  The play 'text' usually encodes this performance by identifying certain kinds of information (and sequence).  This shorthand, or recipe, is interpreted by the actors and scholars.  It is already a program and/or data.  But it is not recognised as either because programs have become associated with computers.  The problem is, no one has asked the right question: if the text is not really concerned with pagination (which is merely an artefact or consequence of wanting to carry it around), then how would we convert it to digital form which is not recreating that but offers some more meaningful medium?  Is it still recognisable as a text?  Why can't we offer an actor just their lines or parts etc?  Why can't we offer a solicitor the consolidated pleadings with a call and response format?  (the 'thread' of pleadings is something like the thread of social media but the formal similarities are not obvious to people).  How does a text record the experience of a flow of conversation?  How is a thread captured?  In some cases, it is by giving an 'id' to a paragraph, but this is not the only way.  It reinforces the scientific.  Law often does this, by having the thread 'refer' back to the paragraph of the preceding document, explicitly, in the text. Thus, law already offers a program, in words, that helps decide how one bit of data follows another.  To 'read' this by a cmoputer that can follow the data is not so difficult, if we prime/prepare it to do so, in context.  And this is made easier if we can write small programs to faciliate text reading, rather than programs that always require custom setups and interfaces just to simulate a paper medium.

# Observations on Benefits

The notional advantages of autotag scripts or programming languages specifically for semantic reasoning about the texts association with them are that they can form the basis of queries: the same information will be returned by the same query, and the human reader substitutes the query for the need to read the whole text. It becomes an information retrieval system that makes sense to the human reader. 

The fact that that any computer-based storage system returns 'correct' information only occurs because the initial categorisation system is designed in such a way that the query links the information.  The 'language' that allows a structured query to return the correct result will mean that the human reader is rewarded with the right information even if the intermediate machine doesn't know anything.  The input code is rewarded with the correct access; the specificity of the input code is based on a combinatorial system that happens to work for categorically-encoded data.  This allows seeminlgy direct access to information rather than sequential access (just like CDs vs tapes or records).

The arts-specific qualities of humanities texts are that they have sets of categorical information uniquely defined by their applications.  What is categorical information may also be significant to humans in a different way; this duality of something unusual to this context which might not be so common in situations from another field.

!!The circular nature of humanities' texts is:
l+
encoding is implicit in the form (e.g. a letter or a play); 
knowing the form (recognising the form) helps with expectations of what is encoded, and how it appears;
a specific example of the form will produce a specific set of variables (e.g. examples of something like 'actors' or 'roles' or 'characters') 
These specific set of variables are joined to something like a line return, or a special character (like a ':') in order create a unique key-delimiter (i.e. by satisfying this pattern of A+b, it thereby becomes a unique delimiter and categorical entry).
the values may appear as the balance of the line, paragraph or other span of text that is defined by some analogue representation.
l-

Using implied analogue delimiters is a way of defining the rules by which human beings designed the structure in a document that does not have simple (or simplest) delimiters.

Adding semantic information is something like adding metadata to a record, except that if it is added in a particular way (i.e. within an environment in which the same semantic data can be interpreted as part of a pattern of use), then it can also form part of a recognised part of data delimitation and characterisation/categorisation.  Pattern recognition is useful in that patterns in context are illustration or applications of rules that appear to be general in that context.  Localised general rules.  This is all that programs are.

# Linguistics

Early linguistic attempts to document languages that were oral experience similar questions: should the character set chosen the first time be the only one?  Should a conventional representation in language be the only one?  SHould we recognise different spelling as equivalent?  How much of the representation is based around converting sounds into defintive symbols for whole words or part words?

# Metrics in analogue media

Even in the paper form texts, lawyers would put marginal numbers in for references, and publishers of plays would insert line numbers into each Act or Scene to enable players or scholars to find and refer to a specific line when they needed.  These are not incorporated into a document that can relate the specific information to a line (unless additional effort is put into encoding the whole document in XML or something).  It is not a native part of word processors, which are not built around the conventionso of a genre.

# Open Universities