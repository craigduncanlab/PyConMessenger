T:Science Writing Notation
B:WhittakerBlakwork
A:Craig Duncan
D:4 August 2024
C:Some exploration of what a humanities programmer, that centred humanities, would actually have to rework in relation to the assumptions of mathematics and computing.  Software that is written for intelligent users of information, not button pushers.
F:PreprocessingMarkup

# Intro

In Rosemary Huisman's article, she called the hypothetical possibility of an origin story with humanities centred computing coming before scientific computing as !! !*Computer Arts*! !!.  In her imagining of that field, she speculated that the text-processing capacity and analysis of language might have preceded numerical number crunching.  She views these things as two fields interacting with one another: two separated disciplines vying for ownership of the technology.

In my view, we can imagine a centering of the humanities as the natural site of computing by removing the bias that exists in computer science today.  However, first we should understand the nature of the bias that already exists in ownership of structure, and how structure relates to symbols and writing.

Computer science is part of the normalising of a culture that grants itself the ownership of the concept of structure.  By this I mean that it has invented its own symbolic system and language inspired by mathematical notations that describe the world in terms of objects dealt with in sets and set members (i.e. objects and groups).  It doesn't begin to make meaning on the basis of the structure of social texts that are humanities texts.  In theory, a language for data could be freely defined by any kind of symbolic system of delimiters and categories that would be used to create sets and groups and differences.  Similar ideas are present in linguistics.  

The humanities deals with texts and punctuation that preceded the invention of both mechanical type and computers.  The same set of alphabetic symbols used in literary manuscripts came to be used in mathematics too, but mathematical proofs and other documents were written with increasingly compact significance attached to those symbols, in such a way that they would always be assumed to be distinguished from ordinary writing.  There is always potential for a collision.  

The division between science and humanities is not some subtle preference, but has influenced the way in which the tools of each have respectively developed (even though they may share a common set of symbols, like the alphabet).  The humanities have used the tool of writing as a record of sounds, and writing has evolved and reflected the priorities and attention of its participants.  In its own way, mathematics used and developed notations that assumed it would appear in its own visual space (when written), protected from interruption and protected from inconsistent signs and symbols.  The notion of 'scope' for a notation system is at work here: within scope, mathematics considers itself a single priority.  Computing takes this notation and works with it, gradually allowing textual information in on its own terms.

The relationship between cultural norms of mathematics and normalisation of mathematical notation, as the centre of structure, can be illustrated by examining how a small set of symbols (the letters, numbers and punctuation are preferentially used in the sciences and mathematics as if they encounter no conflicts.  Even within this notation, it took over from the general use of the alphabet and defined new meanings for them within scope (even appropriating the forward slash for fractions and division).  Though the alphabet existed simultaneously, in terms of significance, mathematics wanted to be ignorant of ordinary texts, languages and things like the conventional use of grammatical punctuation.  A comma as a pause became a comma as a delimiter.  A parenthensis became a group.  The notion of ordinary writing and mathematics drifted apart. 

# Mathematical notion formed the basis of programming languages

Mathematics uses delimiters like commas to separate items on the assumption that this is operating in a different field to literature: the comma is not in danger of being confused with punctuation where no language, sentences and narratives exist.  

Mathematical notation, even before electronic computers, developed with the assumption that the unit of information, structure and groups would not completely ignore the humanities, but not be used in deference to them.  For example, a set would be a notation that made information fit within an abstract form of information where something was defined with reference to the group to which it belonged: mathematics defined relationships of parts and the whole which were abstract observations about data.  !!There is a cultural bias that continues to influence the entire manner of using and operating computers, one as deeply engrained as the languages used, regardless of the levels of abstraction?!!  We can review some history to understand this more fully.

More abstract computer programming languages then evolved or took up the conventions of mathematics mid twentieth century, so that the comma was the basis for defining lists and sets and other data types.  Brackets were used for sets in mathematics on the basis that they were not parenthetical asides but important ways of defining what was contained within a group.  Eventually, when it came to strings of characters, computer programming languages had to invent 'escape' characters, which would then differentiate something like a division sign or a quote that appeared in the literature text from its use in the programming language itself (e.g. where the quote was appropriated as the boundary of a string of characters: at once invoking the metaphor of literature but at the same time appropriating it as the delimiter of the contents and not being used for the computation itself).

# Mathematical notation defined the conventions of text editors

When text editors were invented, the first version of markup (to identify bold and italic text) was itself defined by the norms of mathematics.  

With markup, there was an assumption (if not a conscious choice) that:
l+
it was necessary to define parts of text as data (to partition a text as the object of an action); and
even in the notion of 'data', there was an underlying comparison with type and gutenberg machines: that the data was to be laid out on screen or page for a general audience.  The public nature of data influenced the idea that some marking up had a human audience.
l-

The subject-object distinction within this context was based on dominant operations (verbs) where the computer was the agent for the transform (i.e. it was the machine that performed the operations, and by complex electronic interactions, would eventually supply ink to a page to make this a non-electronic reality). 

Against this background, the choice for markup notation was at least partly inspired by mathematical notation (groupings, start and end tags), but it was also different, in that there was an awareness of the need to avoid collisions with normal punctuation.  This time, the notion of partitioning tags appropriated other symbols from mathematics (e.g. greater and less than signs become angled brackets).  These were, or were assumed to be, less likely to be used within an ordinary text. (cf TeX, a computer language for typesetting mathematical texts, mainly: a text for publication but not for thinking within the humanities).

It is with this background that the concept of the graphical interface and notions of 'what you see is what you get developed'.  People didn't like mathematical notation in their written texts (it didn't look right, and they couldn't see sufficient upside to doing it: if it could be avoided, so much the better).  The graphic formats that became the norm for analogue representations evolved to look more like the final output, and the mathematical notations were only implied, rather than the concern of the user.   The downside of this approach of hiding complexity was that it prevented development of ideas about abstract data types that were actually more suited to the humanities.

# Data formats are still influenced by the priority of mathematical notatons/sciences.

The 'language' and data format for most text is centred on a general layout language that approximates the paradigm of a typesetting machine.  It's not concerned with what the author is attempting to do, or humanities data formats that assist humanities workers manipulate custom data formats inside a computer.  

Any graphical format arrives at its form by mediating the text through sets and structures that prioritise that need for hiding the structure by the layout.   The structure and the analogue representation are two sides of the same outcome: the analogue outcome is not possible without the structure; the structure only exists to serve the analogue outcome, yet in its non-visual form it is still superficially based on a mathematical notation.  

Even with the WYSIWYG approach, the hiding of set-based text classification doesn't change the assumption that mathematical notation is an abstract scheme that focusses on layout and not the semantic concepts that an author or discipline is concerned with (i.e. that mathematical forms are brought to a text in a way which doesn't concern itself with how an author might like to partition the text).  

The legacy of trying to combine mathematically-inspired delimiters and ordinary texts means that many text data formats still use complex forms of delimiters (like the angled brackets hidden away in OOXML files, as used for MS Word). This underlying set-notation for classification of humanities data is not for general use, but exists so that it can be differentially formated for an analogue output (fonts, size and so on).   

Graphical interfaces don't change the data format; they merely hide it in favour of a dumbed-down interface.  Hiding abstract data types in a computer not only removes the possibility of abstract data being used, but it forces the user to be a button-pusher.  Hiding set notation yet achieving simulation of on-screen layout requires positioning narrow text classification schemes as a manual activity: a kind of button pushing exercise.  By encouraging people to use defined styles, mouse tools and selections, writers who were asking for little more than the functions of a typewriter were appeased: they could pretend that the computer scientists and their divergent notations were not present.  Those that didn't want to attend to mathematics and logic directly could be trained up to carry out mechanical motions instead.  

# Writing software as a business for the lowest-common-denominator

Digital gutenberg machines is not such a stretch for the description of how a lot of writing software works. This is not text-editing software for intelligent people, who can speak generally about the kinds of information they want to include/exclude or work with.  Software companies have found that there is an easy market for people that do not care about having control over how the computer records information as a precursor to tasks beyond publication.  The layout software approach mainly provides buttons and levers, and which the writer is both writer and button pusher.  The author becomes the driver of a layout-button-pushing system.  The software is designed with a view to repeating the operation of the machines that were used to make markings on paper.  The fact that we have terms like 'markup' and 'markdown' is itself a strong sign that the focus is on crude making of marks on paper, rather than more abstract kinds of thought.

Each application in the 'suite' of these programs adopts the paradigm that users want to prepare things on screen in their final state.  It compresses workflows, hiding the machinations of data.  A spreadsheet has to look like a book first, rather than being the output of some more intelligent system of storing data, or even using programming as an intermediate step before transferring data to a spreadsheet.  Working backwards, the idea of 'importing' data is brought into these designs as an afterthought.

Writing software is marketed to lazy people who want little more than basic arrangement of words on a page, or calculator-level complexity in the case of spreadsheets.  The neglected users are the kinds of people who are wanting to work with abstract divisions in their writing, and are making observations, and references, and so on, are still being required to press buttons and proceed directly to the 'end look', without opportunities for intermediate reasoning about what they are referring to or using, or taking advantage of a computer's ability to store data in lists and then save labour when it is used repeatedly.

Ironically, creative writers and those that want to mix text and images may well benefit from the ability to leverage patterns in reference data and to reduce the manual work for publication even more, whilst also including some higher-level semantic classifications that have more to do with the author's preferences and less to do with giving attention to layout alone.  Intelligent writers have become the lesser players in a business of making virtual gutenberg machines that aim to replace quills, pens and manual typewriters.  

IN summary, there are several simultaneous moves in the promotion of graphical information interfaces for text editing.  The main effect is that the foregrounding of a button-based, tool-like interface; the secondary effect is that anything like a domain-specific language or logic is centred on layout rather than the author's discipline or interests; the third effect is that this language/logic and consequential data structures (sets, mathematical notation) is hidden from view.  All of these elements must be unpacked and made explicit if we are to more fully appreciate the scope for doing things differently.

# Abstract data types for the humanities

This mathematical notion of data based on mechanical typesetting was for expediency, but it has long delayed consideration of intermediate forms of data, or humanities-specific forms of data, that somehow transcend the mechanical setting of type and relate to the human perceptual system directly (like how text is or is not separated on a page, or how ideas or concepts are quickly distinguished on a page or not).  

The preceding historical concern with making books (and not merely page-based typesetting) has also tended to influence how textual data is conceived.  These are useful for understanding why certain concepts are still retained when people are interested in publication, but they too defer the awareness that humanities projects (literature, law etc) have data structures that can help store, record, query andn manipulate data.  Even adopting data formats like XML is an indirect method of approaching this problem.  ADSF's for humanities (ADSFH) is a subject worthy of its own attention: it will also pave the way for suitable programming concepts and constructs to do more work with less.  This can also be assisted by separating the definition of ADSFH (semantic information) and programming code from texts, where this is useful.  A text may well be an expression of an ADSFH, and thus benefit from its more explicit definition.

What kind of ADS might be useful?  Some abstract data types in computer science, like keys and values, may well be more useful for data representation in the humanities than simple lists and sets.  On the other hand, even more complex data types might be the norm in the humanities, and offer simplification of working with humanities documents.  (e.g. the set of characters for a play, and the dialogue).   These practical forms of data can have defined abstract structures in just the same way as computer scientists define abstract data structures like stacks, or graphs.   What hasn't been embraced is the idea that something like a 'play', when divorced from its layout, can have a data structure too.  The underlying data structure may then find specific forms of expression which enable this pattern to be used.

In what ways should we pay attention to features of plays that can help auto-tag or auto-format plays etc?  We can match patterns based on underlying data structures and avoid the need for manual markup.  Typesetting markup that a writer has to pay attention to ignores the possibility that a computer programming can make use of data types and patterns to save work.

One of the ways in which we can incorporate ADSFH into conventional computer workflows is to use an ADFSH first, then convert it to a more conventional com sci ADS (like a list etc) as a precursor to rendering as a layout like HTML.  However, semantic information can be retained (and in some cases, that semantic information may be enough to then point to the underlying ADSFH structure and/or content).   If a list of items in a legal document was defined as the particulars to a particular topic, then it could be semantically tagged in that way in HTML, but this would merely be an expression of an underlying data format that would have computational reasoning possibilities (an answer to a query, at least).  It should be possible to programmatically explore the data in a legal pleading to obtain responses to particular paragraphs etc.  The goal of writing legal documents should be to easily pave the way for data ingestion into a structure which then allows an interactive interface, or some scripts, to make further use of the data, or provide output as required.  It should not be sufficient to simply purchase a proprietary and graphical interface to data which hides the way in which reasoning is done.  There is no education of professional workers in the humanities as to what their data looks like if we do not have an independent discussion about data structures, programs and queries (independent of any specific type of software etc).

!! A text book is needed so that this discussion can be taken into classrooms and make the humanities more data-savvy : i.e. to discus what they have been doing already, but to bring home that ordinary word processing documents are not suited to purpose; that they lack digital relevance because they are based on a typesetting paradigm not a data structures paradigm.!!

# Writing software that recentres the notion of programming

What does an internal form of relating to computers as a humanities professional or artist look like?  There is a middle ground where a text editor can provide programming languages designed around the nature of the content of texts.  This is one in which computers are appropriated as the tool of the humanities, and ways of structuring information are self-evidently based on analogue histories of formatting literare and other documents.  

Large efforts have to be made to provide tools and scope for working with text that assumes that layout is secondary to structure, not the primary motivation for it.  There needs to be more software examples that demonstrate how a writer can be thinking about structure and know that the computing environment has been designed with intelligent semantic structures assumed to be the norm, not the exception.  (The semantic web paradigm, that subordinates everything to a markup paradigm, doesn't really do this).  

The concept of always putting new marks on a digital page has to be devalued in favour of making more use of ways to interpret the plainest digital forms we can use that retain basic spatial information about the source analogue documents.   e.g. as if we have perfect character recognition, and transfer to a digital file with vertical and horizontal components (commonly called a plain text file). How much can we do to interpret this information with and without supplementary information from the author?  If the means of mapping semantic significance to text can be summarised in a document independently of the source material (the basic digital form), then the separation of concerns helps emphasise just how much the mere transfer of analogue to the simplest form of digital representation can achieve if we can focus on the proper interpretation of that data in that form, by intelligent programming.

Meta-data at the top of a memo transferred to plain text file is one example of a structure apparent on the face of the document.  (!!In a way, even this basic web essay is a kind of memo, in which it commences with information that would be suitable for filing, whether in digital or paper form!!).  A basic transfer of this paper memo to digital form has a simple correspondence in terms of the vertical and horizontal layout of characters, where the 'line length' and 'line end' conventions help create a digital representation within a finite number of character blocks.  The goal should be to extract relevant meaning from this spatial layout before abandoning it.  This then becomes the first digital form that can be analysed for meaning.  If you simply recognise this digital form that is a non-proprietary analogue for paper media, one which already has both classification and partitioning, then it should already represent a data format to humanities computer science.

Better still, by inventing new programming languages that use intermediate-level data abstractions using terms suited to the humanities, it allows more immediate alignment between the concepts in the humanities and computer-based processing.  The difference between programming languages as the centre of new discipline knowledge and a software application like a word processor, is that they contain fewer commercially-influenced assumptions about what data structures or interfaces are preferred, because they are free to avoid considerations of market, consumer knowledge, and immediate popularity. 

Some formats expand on the simplest digital form.  Tools that make semantics explicit in an HTML markup format are adding value, making use of a format that has tags and greater opportunities for labelling than ordinary text.  

# Programming languages to allow authors to define ADSFH

The relationship between different types of data might not be so complicated (it might be a set of lists e.g. a play: characters, scenes, acts, locations, stage directions etc).  Each of these is a group of possibilities that are then given expression through the text.  Within that narrow range of objects or reduced variables, the order or combinations may find different expression.   The data structure is, in a way, a form of expressing a particular state, but it does not have to be.  It might simply be a direction or instruction chosen from a set of possibilities, and thus assume interaction with human beings.  In legal contracts the documents might not only be reducing the world to a set of simpler concepts, but also providing a tool for social relations (in this respect, systemic functional linguistics adopts a similar approach: Halliday, Huisman etc).

The point about defining some of the basic structures might be that they don't have to achieve logical certainty, but simply be enough to help categorise parts of text for recall or publishing.  We don't have to equipt a computer to interpret the meaning of literature if it is sufficient simply to work out (through data splitting, reductive text recognition etc) that a portion of text is dialogue.  This might provide the ability to get at the data without needing a full read or a book form.  It makes the digital format relevant and useful because it can do something that wasn't already available in analogue form (!!a useful test, generally, of the computability of a document.!!)

# Production system software

In legal production systems where texts are converted into products, and information becomes variables, and variables are modified based on conditions, the idea of a 'program' is central to the work process.  Yet the cultural norm is not to implement workflows by writing programs (at least, not consciously), but to give people templates and administrative tasks.  The design of workflows is as much influenced by a conservative desire to lock-down choice so that the end result is mechanical (more of a type-setting machine than freeform reasoning within the domain).

My interest in how production software is related to language and the process of writing can probably be summarised by what I have written in relation to /SystemicFunctionalLinguistics/.

The fact that humanities professionals are involved often means that in conjunction with software vendors or developers, they will settle on elaborate graphical software interfaces full of buttons and choices.  The 'language' of GUI interfaces is ultimately working on inputs and outputs like anything else.  It provides a paradigm through which user interaction proceeds that is likely to be more static than other options.

The form-like GUI is part of an effort to simplify the tool to a level that it becomes little more than button-pushing administration of inputs and outputs; perhaps little more than a type-setting tool with word options.   This rigid, machine-like facade with box-like architecture portrays something that is safer because it is inflexible.  It also has structure and solidity built into its aesthetic: the non-alterable parts of the forms are greyed in like concrete, and the white-backed backgrounds of text boxes where variables and choices are inserted seems to be little more than a post-box for input.   The concrete aesthetic resists the paradigm of writing and space as a means to effect communication; it even resists the mathematical statement of choices and logic.

(even  programming templates that are relatively succesful and retained for repetition involve closing down the flexiblity, but they do not appear that way: there is always reserved the ability to rework those instructions).

In trancript writing, there is an underlying/establish manual or style guide that assists with the inclusion of rigid and patterned word formulas into the text.  These also illustrate that specification of patterns of input can be achieved as much through text presentation and editing as much as any formal window with buttons and components.  

A style manual is, in its own way, the definition of an abstract data type, and so if this can be successfully described and parsed in digital form, it allows milestone parts of a text to be located and used.